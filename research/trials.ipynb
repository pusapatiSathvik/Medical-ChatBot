{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc357516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S:\\\\Projects\\\\Medical-ChatBot'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caadf738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"S:\\Projects\\Medical-ChatBot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d90b3d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f655d2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from PDF files\n",
    "def load_pdf_files(data):\n",
    "    loader = DirectoryLoader(\n",
    "        data,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58332c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf_files(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d68a24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "637"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68da2d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def filter_to_minimal_docs(docs: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Given a list of Document objects, return a new list of Document objects\n",
    "    containing only 'source' in metadata and the original page_content.\n",
    "    \"\"\"\n",
    "    minimal_docs: List[Document] = []\n",
    "    for doc in docs:\n",
    "        src = doc.metadata.get(\"source\")\n",
    "        minimal_docs.append(\n",
    "            Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={\"source\": src}\n",
    "            )\n",
    "        )\n",
    "    return minimal_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96ba9047",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_docs = filter_to_minimal_docs(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70158579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "637"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (minimal_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3313421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the documents into smaller chunks\n",
    "def text_split(minimal_docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=20,\n",
    "    )\n",
    "    texts_chunk = text_splitter.split_documents(minimal_docs)\n",
    "    return texts_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eca7ae54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 5859\n"
     ]
    }
   ],
   "source": [
    "texts_chunk = text_split(minimal_docs)\n",
    "print(f\"Number of chunks: {len(texts_chunk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06749de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aaedec8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sathw\\AppData\\Local\\Temp\\ipykernel_16012\\30168171.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def download_embeddings():\n",
    "    \"\"\"\n",
    "    Download and return the HuggingFace embeddings model.\n",
    "    \"\"\"\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "embedding = download_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c19a8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48c35680",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = embedding.embed_query(\"Hello Embedding Model\")\n",
    "# vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71de3ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector length: 384\n"
     ]
    }
   ],
   "source": [
    "print( \"Vector length:\", len(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bffc529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e494a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2c8f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone \n",
    "pinecone_api_key = PINECONE_API_KEY\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "618e8405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.pinecone.Pinecone at 0x263d232b9d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0dfd1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    " \n",
    "\n",
    "index_name = \"medical-chatbot\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name = index_name,\n",
    "        dimension=384,  # Dimension of the embeddings\n",
    "        metric= \"cosine\",  # Cosine similarity\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14788fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=texts_chunk,\n",
    "    embedding=embedding,\n",
    "    index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32bbf336",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe3d4244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='f0732e42-1470-4ea1-b9ce-742de9bd3f50', metadata={'source': 'data\\\\Medical_book.pdf'}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'),\n",
       " Document(id='3f2a93c1-9673-43dd-b44e-bb1fbb807549', metadata={'source': 'data\\\\Medical_book.pdf'}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'),\n",
       " Document(id='9001c769-5ca4-4385-a022-1bc5ef0fc74c', metadata={'source': 'data\\\\Medical_book.pdf'}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 2 25\\nAcne\\nAcne vulgaris affecting a womanâ€™s face. Acne is the general\\nname given to a skin disorder in which the sebaceous\\nglands become inflamed. (Photograph by Biophoto Associ-\\nates, Photo Researchers, Inc. Reproduced by permission.)\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 25')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is Acne?\")\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "711898bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35333d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (GEMINI_API_KEY) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85709231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI learns patterns from data to make decisions or perform tasks.\n"
     ]
    }
   ],
   "source": [
    "# from google import genai\n",
    "\n",
    "# # The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "# client = genai.Client()\n",
    "\n",
    "# response = client.models.generate_content(\n",
    "#     model=\"gemini-2.5-flash\", contents=\"Explain how AI works in a few words\"\n",
    "# )\n",
    "# print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af18a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import create_retrieval_chain\n",
    "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "# from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db0322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt = (\n",
    "#     \"You are an Medical assistant for question-answering tasks. \"\n",
    "#     \"Use the following pieces of retrieved context to answer \"\n",
    "#     \"the question. If you don't know the answer, say that you \"\n",
    "#     \"don't know. Use three sentences maximum and keep the \"\n",
    "#     \"answer concise.\"\n",
    "#     \"\\n\\n\"\n",
    "#     \"{context}\"\n",
    "# )\n",
    "\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", system_prompt),\n",
    "#         (\"human\", \"{input}\"),\n",
    "#     ]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c2760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_answer_chain = create_stuff_documents_chain(chatModel, prompt)\n",
    "# rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4572cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = rag_chain.invoke({\"input\": \"what is Acromegaly and gigantism?\"})\n",
    "# print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22932a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# from langchain_pinecone import PineconeVectorStore\n",
    "# from langchain_classic.chains import create_retrieval_chain\n",
    "# from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# # 1. Configuration\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GEMINI_API_KEY\")\n",
    "# os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# # 2. Initialize Gemini 1.5 Flash\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n",
    "\n",
    "# # 3. Define the Prompt (Medical Assistant Persona)\n",
    "# system_prompt = (\n",
    "#     \"You are a professional Medical Assistant. \"\n",
    "#     \"Use the provided context to answer the user's question accurately. \"\n",
    "#     \"If the answer isn't in the context, state that you don't know. \"\n",
    "#     \"Limit your response to three concise sentences.\"\n",
    "#     \"\\n\\n\"\n",
    "#     \"Context: {context}\"\n",
    "# )\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", system_prompt),\n",
    "#     (\"human\", \"{input}\"),\n",
    "# ])\n",
    "\n",
    "# # 4. Create the RAG Chain\n",
    "# # Using your existing 'retriever' from Pinecone\n",
    "# combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "# rag_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "# # 5. Run the Query\n",
    "# query = \"What is Acne and how is it treated?\"\n",
    "# response = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "# print(f\"Medical Assistant: {response['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f6e8590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Medical Assistant Response ---\n",
      "Acne is a general name for a skin disorder. This condition occurs when the sebaceous glands become inflamed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# 1. Setup API Key\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# 2. Initialize the Model\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "# 3. Retrieval Step (Using your existing Pinecone retriever)\n",
    "query = \"What is Acne?\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "# Combine the content of the retrieved documents\n",
    "context_text = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# 4. Merged Prompt Logic\n",
    "# We translate the LangChain system prompt into a structured input for Gemini\n",
    "prompt = f\"\"\"\n",
    "You are a Medical assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: \n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# 5. Generate Response\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "print(\"--- Medical Assistant Response ---\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee5227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
